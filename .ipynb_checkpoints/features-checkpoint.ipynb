{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold, cross_val_predict\n",
    "\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "from pyfm import pylibfm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (4209, 379)\n",
      "Test shape :  (4209, 378)\n",
      "time: 439 ms\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "train['test'] = 0\n",
    "test['test'] = 1\n",
    "\n",
    "df = pd.concat([train, test], axis=0)\n",
    "y = train.y\n",
    "df = df.drop([\"ID\", \"y\"], axis=1)\n",
    "print(\"Train shape : \", train.shape)\n",
    "print(\"Test shape : \", test.shape)\n",
    "\n",
    "ix_train = df.test == 0\n",
    "ix_test = df.test == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.2 ms\n"
     ]
    }
   ],
   "source": [
    "feat_all = train.columns[2:]\n",
    "feat_categ = train.dtypes.index[train.dtypes == 'object']\n",
    "feat_numb = list(set(feat_all) - set(feat_categ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переменные, которые сильнее всего разделяют средний y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.65 ms\n"
     ]
    }
   ],
   "source": [
    "predictors = ['X339', 'X236', 'X205', 'X204', 'X270', 'X167', 'X277', 'X278', 'X269', 'X29',\n",
    " 'X232', 'X279', 'X263', 'X276', 'X328', 'X272', 'X166', 'X17', 'X382', 'X378',\n",
    " 'X371', 'X108', 'X325', 'X136', 'X54', 'X76', 'X185', 'X162', 'X159', 'X310', 'X252']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.3 ms\n"
     ]
    }
   ],
   "source": [
    "def check_model(predictors):\n",
    "    classifier = lambda: SGDRegressor(\n",
    "        loss='squared_loss',\n",
    "        penalty='elasticnet',\n",
    "        alpha=0.0005,\n",
    "        l1_ratio=0.15,\n",
    "        fit_intercept=True,\n",
    "        n_iter=5,\n",
    "        shuffle=True,\n",
    "        verbose=0, epsilon=0.1, random_state=42, learning_rate='invscaling',\n",
    "        eta0=0.01, power_t=0.25, warm_start=False, average=False)\n",
    "    \n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        #('ss', StandardScaler()),\n",
    "        ('en', classifier())\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        'en__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.02, 0.1, 0.5, 0.6, 0.7, 0.9, 1],\n",
    "        'en__l1_ratio': [0, 0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 0.9, 1]\n",
    "    }\n",
    "    #parameters = {\n",
    "    #    'en__alpha': [0.00001, 0.0001],\n",
    "    #    'en__l1_ratio': [0, 0.0001, 0.001]\n",
    "    #}\n",
    "\n",
    "    folder = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters,\n",
    "        scoring='r2',\n",
    "        cv=folder, \n",
    "        n_jobs=-1, \n",
    "        verbose=1)\n",
    "    grid_search = grid_search.fit(df[ix_train][predictors], \n",
    "                                  y)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.35 ms\n"
     ]
    }
   ],
   "source": [
    "def pickle_check_model(predictors, model_file, bremove = False):\n",
    "    model_file = './../tmp/' + model_file\n",
    "    if bremove and os.path.isfile(model_file):\n",
    "        print(\"Removing old file\")\n",
    "        os.remove(model_file)\n",
    "    if not os.path.isfile(model_file):\n",
    "        print(\"Make grid search...\")\n",
    "        model = check_model(predictors)\n",
    "\n",
    "        with open(model_file, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    else:\n",
    "        with open(model_file, 'rb') as f:\n",
    "            print(\"Load from file...\")\n",
    "            model = pickle.load(f)\n",
    "    print(model.best_score_)\n",
    "    print(model.best_params_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from file...\n",
      "0.132319948991\n",
      "{'en__alpha': 1e-05, 'en__l1_ratio': 0}\n",
      "time: 2.19 ms\n"
     ]
    }
   ],
   "source": [
    "t = pickle_check_model(predictors, '1_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from file...\n",
      "0.132319948991\n",
      "{'en__alpha': 1e-05, 'en__l1_ratio': 0}\n",
      "time: 2.34 ms\n"
     ]
    }
   ],
   "source": [
    "t = pickle_check_model(predictors, '1.1_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 35.8 ms\n"
     ]
    }
   ],
   "source": [
    "n_comp = 5\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_train = ica.fit_transform(df[ix_train][predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.7 ms\n"
     ]
    }
   ],
   "source": [
    "feat_ica = ['ica_' + str(c) for c in np.arange(n_comp)]\n",
    "df_tmp=pd.DataFrame(ica2_results_train, columns=feat_ica)\n",
    "if np.any(train.columns.isin(feat_ica)):\n",
    "    train = train.drop(feat_ica, axis=1)\n",
    "train = pd.concat((train, df_tmp), axis=1)\n",
    "train.shape\n",
    "train.columns\n",
    "\n",
    "del df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from file...\n",
      "0.132328312484\n",
      "{'en__alpha': 1e-05, 'en__l1_ratio': 0}\n",
      "time: 2.54 ms\n"
     ]
    }
   ],
   "source": [
    "t = pickle_check_model(predictors + feat_ica, '2_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from file...\n",
      "-0.00178350274056\n",
      "{'en__alpha': 1e-05, 'en__l1_ratio': 0}\n",
      "time: 2.14 ms\n"
     ]
    }
   ],
   "source": [
    "t = pickle_check_model(feat_ica, '3_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old file\n",
      "Make grid search...\n",
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-1)]: Done 488 out of 495 | elapsed:   25.5s remaining:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.511579804396\n",
      "{'en__alpha': 1e-05, 'en__l1_ratio': 1}\n",
      "time: 26.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 495 out of 495 | elapsed:   25.7s finished\n"
     ]
    }
   ],
   "source": [
    "t = pickle_check_model(feat_numb, '4_model.pkl', bremove=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subaevdi/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIuJzKmoREZ9TUYuI+JyKWkTE5/4/Wd7G3dmko9wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b33bfe7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 244 ms\n"
     ]
    }
   ],
   "source": [
    "a = t.best_estimator_.steps[0][1]\n",
    "df_tmp = pd.DataFrame({'col':feat_numb, 'val':a.coef_})\n",
    "df_tmp['abs_val'] = np.abs(df_tmp.val)\n",
    "df_tmp = df_tmp.sort_values('abs_val', ascending=False).reset_index()\n",
    "plt.plot(df_tmp.abs_val)\n",
    "\n",
    "feat_numb_top150 =list(df_tmp.col[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.51 ms\n"
     ]
    }
   ],
   "source": [
    "def woker(p):\n",
    "    f1 = p[0]\n",
    "    f2 = p[1]\n",
    "    nf = f1 + '_' + f2\n",
    "    train[nf] = train[f1] * train[f2]\n",
    "    return nf\n",
    "\n",
    "numb_pairs = [(f1,f2) for f1, f2 in itertools.combinations(feat_numb_top150, r=2)][:50000]\n",
    "\n",
    "#with multiprocessing.Pool() as pool: # default is optimal number of processes\n",
    "#    feat_numb_pair = pool.map(woker, numb_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#t = pickle_check_model(feat_numb_pair, '5_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Генерим пары по 200 штук\n",
    "2. Добавляем в train\n",
    "3. Запускаем модель из базовыйх + добавленные 100 раз\n",
    "4. Находим лучшую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from file...\n",
      "Load from file...\n",
      "time: 9.26 ms\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('./../tmp/model_score_list.pkl'):\n",
    "    with open('./../tmp/model_score_list.pkl', 'wb') as f:\n",
    "        pickle.dump(model_score_list, f)\n",
    "    with open('./../tmp/model_pairs_list.pkl', 'wb') as f:\n",
    "        pickle.dump(model_pairs_list, f)\n",
    "else:\n",
    "    with open('./../tmp/model_pairs_list.pkl', 'rb') as f:\n",
    "        print(\"Load from file...\")\n",
    "        model_pairs_list = pickle.load(f)\n",
    "    with open('./../tmp/model_score_list.pkl', 'rb') as f:\n",
    "        print(\"Load from file...\")\n",
    "        model_score_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ 10 индексов, R2 в массиве с парами:          scr\n",
      "23  0.442851\n",
      "10  0.436912\n",
      "36  0.436789\n",
      "40  0.436373\n",
      "2   0.433045\n",
      "38  0.430571\n",
      "39  0.430131\n",
      "30  0.429575\n",
      "21  0.429495\n",
      "32  0.428968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subaevdi/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7v8D7s9fkSR+spAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b33cb1320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 224 ms\n"
     ]
    }
   ],
   "source": [
    "df_tmp = pd.DataFrame({'scr':model_score_list})\n",
    "plt.plot(df_tmp.sort_values('scr', ascending=False).values)\n",
    "plt.title(\"Ошибка R2 для грид серча по парам\")\n",
    "print(\"Топ 10 индексов, R2 в массиве с парами:\", df_tmp.sort_values('scr', ascending=False).\\\n",
    "      head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "def woker_split(p_in):\n",
    "    p = p_in.split(\"_\")\n",
    "    f1 = p[0]\n",
    "    f2 = p[1]\n",
    "    df[p_in] = df[f1] * df[f2]\n",
    "    return p_in\n",
    "\n",
    "top_index = df_tmp.sort_values('scr', ascending=False).\\\n",
    "      head(5).index\n",
    "#for pairs_index in \n",
    "#_ = [woker_split(p) for p in model_pairs_list[pairs_index]]\n",
    "\n",
    "feat_pairs_in_top = sum([model_pairs_list[ind] for ind in top_index], [])\n",
    "feat_pairs_in_top = list(set(feat_pairs_in_top))\n",
    "\n",
    "# Add pairs to train\n",
    "_ = [woker_split(p) for p in feat_pairs_in_top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from file...\n",
      "0.486810358613\n",
      "{'en__alpha': 1e-05, 'en__l1_ratio': 1}\n",
      "time: 2.77 ms\n"
     ]
    }
   ],
   "source": [
    "# Построим модель соеденив\n",
    "m = pickle_check_model(feat_numb_top150 + feat_pairs_in_top, '7_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit XGBoost\n",
    "Сделаем сабмит по парам индесков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " ()# mmm, xgboost, loved by everyone ^-^\n",
    "import xgboost as xgb\n",
    "\n",
    "y_mean= np.mean(train.y)\n",
    "# prepare dict of params for xgboost to run with\n",
    "xgb_params = { \n",
    "    'eta': 0.005,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.7,\n",
    "    'min_child_weight': 25,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1,\n",
    "    'tree_method':'hist',\n",
    "    'seed':42\n",
    "}\n",
    "\n",
    "\n",
    "# form DMatrices for Xgboost training\n",
    "dtrain = xgb.DMatrix(train[feat_numb_top150 + top_pairs], train.y)\n",
    "dtest = xgb.DMatrix(test[feat_numb_top150 + top_pairs])\n",
    "\n",
    "# xgboost, cross-validation\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   num_boost_round=1500, # increase to have better results (~700)\n",
    "                   early_stopping_rounds=50,\n",
    "                   verbose_eval=50, \n",
    "                   show_stdv=False\n",
    "                  )\n",
    "\n",
    "num_boost_rounds = len(cv_result)\n",
    "print(num_boost_rounds)\n",
    "\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "print(r2_score(dtrain.get_label(), model.predict(dtrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = model.predict(dtest)\n",
    "res = pd.DataFrame({'ID':test.ID, 'y':y_test})\n",
    "res.to_csv(\"../submit/xgb_feat_numb_top150_top_pairsLB0.54214.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-5439d5a55df6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeat_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_numb_top150\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtop_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_comp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_pairs' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.3 ms\n"
     ]
    }
   ],
   "source": [
    "feat_list = feat_numb_top150 + top_pairs\n",
    "n_comp = 10\n",
    "\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "pca.fit(df[ix_train][feat_list])\n",
    "pca2_df = pca.transform(df[feat_list])\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42, max_iter = 600)\n",
    "ica.fit(df[ix_train][feat_list])\n",
    "ica2_df= ica.transform(df[feat_list])\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "feat_pca = list()\n",
    "feat_ica = list()\n",
    "for i in range(1, n_comp+1):\n",
    "    df['pca_' + str(i)] = pca2_df[:,i-1]\n",
    "    df['ica_' + str(i)] = ica2_df[:,i-1]\n",
    "    feat_pca.append('pca_' + str(i))\n",
    "    feat_ica.append('ica_' + str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_mean= np.mean(train.y)\n",
    "# prepare dict of params for xgboost to run with\n",
    "xgb_params = { \n",
    "    'eta': 0.005,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.6,\n",
    "    'min_child_weight': 25,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1,\n",
    "    'tree_method':'hist',\n",
    "    'seed':42\n",
    "}\n",
    "\n",
    "\n",
    "# form DMatrices for Xgboost training\n",
    "dtrain = xgb.DMatrix(train[feat_pca + feat_ica], train.y)\n",
    "dtest = xgb.DMatrix(test[feat_pca + feat_ica])\n",
    "\n",
    "# xgboost, cross-validation\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   num_boost_round=1500, # increase to have better results (~700)\n",
    "                   early_stopping_rounds=50,\n",
    "                   verbose_eval=50, \n",
    "                   show_stdv=False\n",
    "                  )\n",
    "\n",
    "num_boost_rounds = len(cv_result)\n",
    "print(num_boost_rounds)\n",
    "\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "print(r2_score(dtrain.get_label(), model.predict(dtrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = pickle_check_model(feat_numb_top150 + model_pairs_list[23], '6_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs_in_model = list()\n",
    "for i in [23, 10, 36, 40, 2, 38, 39, 30, 21, 32]:\n",
    "    a = m.best_estimator_.steps[0][1].coef_\n",
    "    df_tmp = pd.DataFrame({'col':feat_numb_top150 + model_pairs_list[i], 'val':a})\n",
    "    df_tmp['abs_val'] = np.abs(df_tmp.val)\n",
    "    df_tmp = df_tmp.sort_values('abs_val', ascending=False).reset_index()\n",
    "    pairs_in_model.append([x for x in df_tmp.col[:50] if '_' in x])\n",
    "    #plt.plot(df_tmp.abs_val)\n",
    "    #plt.title(str(i))\n",
    "    #plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = sum(pairs_in_model, [])\n",
    "feat_pairs_in_top = list(np.unique(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = pickle_check_model(feat_numb_top150 + feat_pairs_in_top, '7_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = pickle_check_model(feat_numb + pairs_in_top, '8_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = m.best_estimator_.steps[0][1].coef_\n",
    "df_tmp = pd.DataFrame({'col':feat_numb + pairs_in_top, 'val':a})\n",
    "df_tmp['abs_val'] = np.abs(df_tmp.val)\n",
    "df_tmp = df_tmp.sort_values('abs_val', ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двух этапный SGD\n",
    "1. Обучем SGD на feat_numb\n",
    "2. Вычислим ошибку $$y_{err} = y - y_{hat}$$\n",
    "3. Обучим $y_{pairs} = SGD(feat\\_pairs, y_{err})$\n",
    "4. Конченый прогноз\n",
    "$$y_{end} = y_{hat} + y_{pairs} $$\n",
    "\n",
    "$$y_{hat} =w_0 + w*X_{feat\\_numb}, min(Error(y, y_{hat})) $$\n",
    "$$y_{pairs} =z_0 + z*X_{feat\\_pairs} , min(Error( y - y_{hat}, y_{pairs} ))$$\n",
    "\n",
    "$$ y_{hat} + y_{pairs} = w_0 + w*X_{feat\\_numb} + z_0 + z*X_{feat\\_pairs} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "clf = SGDRegressor(\n",
    "        loss='squared_loss',\n",
    "        penalty='elasticnet',\n",
    "        alpha=1e-05,\n",
    "        l1_ratio=1,\n",
    "        fit_intercept=True,\n",
    "        n_iter=5,\n",
    "        shuffle=True,\n",
    "        verbose=0, epsilon=0.1, random_state=42, learning_rate='invscaling',\n",
    "        eta0=0.01, power_t=0.25, warm_start=False, average=False)\n",
    "y_hat = cross_val_predict(clf, train[feat_numb], y=train.y, cv=kf)\n",
    "r2_score(train.y, y_hat)\n",
    "\n",
    "y_err = train.y - y_hat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8418, 211), (8418, 1768), (8418, 1979))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "\n",
    "for col in feat_categ:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "X = OneHotEncoder().fit_transform(df[feat_categ])\n",
    "\n",
    "\n",
    "feat_ohe = ['ohe_' + str(i) for i in np.arange(X.shape[1])]\n",
    "t = pd.DataFrame(X.todense(), columns=feat_ohe, index=df.index)\n",
    "df_2 = pd.concat( [df, t] , axis=1)\n",
    "\n",
    "\n",
    "\n",
    "t.shape, df.shape, df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_mean= np.mean(y)\n",
    "# prepare dict of params for xgboost to run with\n",
    "xgb_params = { \n",
    "    'eta': 0.005,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.8,\n",
    "    'min_child_weight': 25,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1,\n",
    "    'tree_method':'hist',\n",
    "    'seed':42,\n",
    "#    'gamma': 0.05,\n",
    "    'alpha':0.3\n",
    "}\n",
    "\n",
    "tmp = normalize(df_2[feat_ohe + feat_numb + feat_ica])\n",
    "\n",
    "# form DMatrices for Xgboost training\n",
    "dtrain = xgb.DMatrix(tmp[ix_train], y)\n",
    "dtest = xgb.DMatrix(tmp[ix_test])\n",
    "\n",
    "# xgboost, cross-validation\n",
    "cv_result = xgb.cv(xgb_params, \n",
    "                   dtrain, \n",
    "                   num_boost_round=1500, # increase to have better results (~700)\n",
    "                   early_stopping_rounds=100,\n",
    "                   verbose_eval=200, \n",
    "                   show_stdv=False\n",
    "                  )\n",
    "\n",
    "num_boost_rounds = len(cv_result)\n",
    "print(num_boost_rounds)\n",
    "\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "print(r2_score(dtrain.get_label(), model.predict(dtrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_result.iloc[-1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = pd.DataFrame(X.todense())\n",
    "# = pd.concat([df,t], axis=1)\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd.concat([train,test], axis=0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "y_categ = cross_val_predict(clf, X, y=y_err, cv=kf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2_score(train.y, y_categ + y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_model2(X, y):\n",
    "    classifier = lambda: SGDRegressor(\n",
    "        loss='squared_loss',\n",
    "        penalty='elasticnet',\n",
    "        alpha=0.0005,\n",
    "        l1_ratio=0.15,\n",
    "        fit_intercept=True,\n",
    "        n_iter=5,\n",
    "        shuffle=True,\n",
    "        verbose=0, epsilon=0.1, random_state=42, learning_rate='invscaling',\n",
    "        eta0=0.01, power_t=0.25, warm_start=False, average=False)\n",
    "    \n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        #('ss', StandardScaler()),\n",
    "        ('en', classifier())\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        'en__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.02, 0.1, 0.5, 0.6, 0.7, 0.9, 1],\n",
    "        'en__l1_ratio': [0, 0.001, 0.01, 0.1, 0.3, 0.5, 0.75, 0.9, 1]\n",
    "    }\n",
    "    #parameters = {\n",
    "    #    'en__alpha': [0.00001, 0.0001],\n",
    "    #    'en__l1_ratio': [0, 0.0001, 0.001]\n",
    "    #}\n",
    "\n",
    "    folder = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        parameters,\n",
    "        scoring='r2',\n",
    "        cv=folder, \n",
    "        n_jobs=-1, \n",
    "        verbose=1)\n",
    "    grid_search = grid_search.fit(X, y)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = check_model2(X, y_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## FFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.89 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subaevdi/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [\n",
    "    {\"user\": \"1\", \"item\": \"5\", \"age\": 19},\n",
    "    {\"user\": \"2\", \"item\": \"43\", \"age\": 33},\n",
    "    {\"user\": \"3\", \"item\": \"20\", \"age\": 55},\n",
    "    {\"user\": \"4\", \"item\": \"10\", \"age\": 20},\n",
    "]\n",
    "v = DictVectorizer()\n",
    "X = v.fit_transform(train)\n",
    "print(X.toarray())\n",
    "\n",
    "\n",
    "y = np.repeat(1.0,X.shape[0])\n",
    "fm = pylibfm.FM()\n",
    "fm.fit(X,y)\n",
    "fm.predict(v.transform({\"user\": \"1\", \"item\": \"10\", \"age\": 24}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 267 ms\n"
     ]
    }
   ],
   "source": [
    "tmp = normalize(\n",
    "    csr_matrix(df_2[ix_train][feat_numb_top150 + feat_pairs_in_top].astype(float))\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(tmp, np.array(y),\n",
    "                                                    test_size=0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_num_factors 1 _num_iter 2 0.001 0.475593144494\n",
      "_num_factors 1 _num_iter 2 0.01 0.393411655344\n",
      "_num_factors 1 _num_iter 10 0.001 0.536195544132\n",
      "_num_factors 1 _num_iter 10 0.01 0.441352527222\n",
      "_num_factors 1 _num_iter 30 0.001 0.530531658617\n",
      "_num_factors 1 _num_iter 30 0.01 0.432089819451\n",
      "_num_factors 1 _num_iter 50 0.001 0.534202621755\n",
      "_num_factors 1 _num_iter 50 0.01 0.443459519193\n",
      "_num_factors 1 _num_iter 100 0.001 0.535219453712\n",
      "_num_factors 1 _num_iter 100 0.01 0.424165981271\n",
      "_num_factors 2 _num_iter 2 0.001 0.495771406381\n",
      "_num_factors 2 _num_iter 2 0.01 0.338617017313\n",
      "_num_factors 2 _num_iter 10 0.001 0.460684388528\n",
      "_num_factors 2 _num_iter 10 0.01 0.494094025908\n",
      "_num_factors 2 _num_iter 30 0.001 0.474809820309\n",
      "_num_factors 2 _num_iter 30 0.01 0.501676074092\n",
      "_num_factors 2 _num_iter 50 0.001 0.515468683133\n",
      "_num_factors 2 _num_iter 50 0.01 0.512112401949\n",
      "_num_factors 2 _num_iter 100 0.001 0.532584219263\n",
      "_num_factors 2 _num_iter 100 0.01 0.48852910494\n",
      "_num_factors 3 _num_iter 2 0.001 0.414516619313\n",
      "Exception\n",
      "_num_factors 3 _num_iter 2 0.01 inf\n",
      "_num_factors 3 _num_iter 10 0.001 0.546238067588\n",
      "Exception\n",
      "_num_factors 3 _num_iter 10 0.01 inf\n",
      "_num_factors 3 _num_iter 30 0.001 0.559299716719\n",
      "Exception\n",
      "_num_factors 3 _num_iter 30 0.01 inf\n",
      "_num_factors 3 _num_iter 50 0.001 0.556605394467\n",
      "Exception\n",
      "_num_factors 3 _num_iter 50 0.01 inf\n",
      "_num_factors 3 _num_iter 100 0.001 0.558210837083\n",
      "Exception\n",
      "_num_factors 3 _num_iter 100 0.01 inf\n",
      "_num_factors 4 _num_iter 2 0.001 0.483401055824\n",
      "Exception\n",
      "_num_factors 4 _num_iter 2 0.01 inf\n",
      "_num_factors 4 _num_iter 10 0.001 0.551366538094\n",
      "Exception\n",
      "_num_factors 4 _num_iter 10 0.01 inf\n",
      "_num_factors 4 _num_iter 30 0.001 0.539814160829\n",
      "Exception\n",
      "_num_factors 4 _num_iter 30 0.01 inf\n",
      "_num_factors 4 _num_iter 50 0.001 0.546138476455\n",
      "Exception\n",
      "_num_factors 4 _num_iter 50 0.01 inf\n",
      "_num_factors 4 _num_iter 100 0.001 0.506406002506\n",
      "Exception\n",
      "_num_factors 4 _num_iter 100 0.01 inf\n",
      "_num_factors 5 _num_iter 2 0.001 0.478440048835\n",
      "Exception\n",
      "_num_factors 5 _num_iter 2 0.01 inf\n",
      "_num_factors 5 _num_iter 10 0.001 0.54169334923\n",
      "Exception\n",
      "_num_factors 5 _num_iter 10 0.01 inf\n",
      "_num_factors 5 _num_iter 30 0.001 0.5499910714\n",
      "Exception\n",
      "_num_factors 5 _num_iter 30 0.01 inf\n",
      "_num_factors 5 _num_iter 50 0.001 0.527882558617\n",
      "Exception\n",
      "_num_factors 5 _num_iter 50 0.01 inf\n",
      "_num_factors 5 _num_iter 100 0.001 0.546234051462\n",
      "Exception\n",
      "_num_factors 5 _num_iter 100 0.01 inf\n",
      "_num_factors 6 _num_iter 2 0.001 0.507467933117\n",
      "Exception\n",
      "_num_factors 6 _num_iter 2 0.01 inf\n",
      "_num_factors 6 _num_iter 10 0.001 0.463598383886\n",
      "Exception\n",
      "_num_factors 6 _num_iter 10 0.01 inf\n",
      "_num_factors 6 _num_iter 30 0.001 0.478502686589\n",
      "Exception\n",
      "_num_factors 6 _num_iter 30 0.01 inf\n",
      "_num_factors 6 _num_iter 50 0.001 0.53857266744\n",
      "Exception\n",
      "_num_factors 6 _num_iter 50 0.01 inf\n",
      "_num_factors 6 _num_iter 100 0.001 0.552906511468\n",
      "Exception\n",
      "_num_factors 6 _num_iter 100 0.01 inf\n",
      "_num_factors 7 _num_iter 2 0.001 0.495040005676\n",
      "Exception\n",
      "_num_factors 7 _num_iter 2 0.01 inf\n",
      "_num_factors 7 _num_iter 10 0.001 0.491009876265\n",
      "Exception\n",
      "_num_factors 7 _num_iter 10 0.01 inf\n",
      "_num_factors 7 _num_iter 30 0.001 0.402911148159\n",
      "Exception\n",
      "_num_factors 7 _num_iter 30 0.01 inf\n",
      "_num_factors 7 _num_iter 50 0.001 0.55243240816\n",
      "Exception\n",
      "_num_factors 7 _num_iter 50 0.01 inf\n",
      "_num_factors 7 _num_iter 100 0.001 0.542882435864\n",
      "Exception\n",
      "_num_factors 7 _num_iter 100 0.01 inf\n",
      "_num_factors 8 _num_iter 2 0.001 0.445515730601\n",
      "_num_factors 8 _num_iter 2 0.01 0.283978032444\n",
      "_num_factors 8 _num_iter 10 0.001 0.403085032267\n",
      "_num_factors 8 _num_iter 10 0.01 0.398136582731\n",
      "_num_factors 8 _num_iter 30 0.001 0.336637254834\n",
      "_num_factors 8 _num_iter 30 0.01 0.379166809336\n",
      "_num_factors 8 _num_iter 50 0.001 0.481284719227\n",
      "_num_factors 8 _num_iter 50 0.01 0.444569638831\n",
      "_num_factors 8 _num_iter 100 0.001 0.477546662526\n",
      "_num_factors 8 _num_iter 100 0.01 0.449570716664\n",
      "_num_factors 9 _num_iter 2 0.001 0.494966385437\n",
      "Exception\n",
      "_num_factors 9 _num_iter 2 0.01 inf\n",
      "_num_factors 9 _num_iter 10 0.001 0.419114996323\n",
      "Exception\n",
      "_num_factors 9 _num_iter 10 0.01 inf\n",
      "_num_factors 9 _num_iter 30 0.001 0.500869968142\n",
      "Exception\n",
      "_num_factors 9 _num_iter 30 0.01 inf\n",
      "_num_factors 9 _num_iter 50 0.001 0.524825942661\n",
      "Exception\n",
      "_num_factors 9 _num_iter 50 0.01 inf\n",
      "_num_factors 9 _num_iter 100 0.001 0.527706241629\n",
      "Exception\n",
      "_num_factors 9 _num_iter 100 0.01 inf\n",
      "_num_factors 10 _num_iter 2 0.001 0.226239233867\n",
      "Exception\n",
      "_num_factors 10 _num_iter 2 0.01 inf\n",
      "_num_factors 10 _num_iter 10 0.001 0.482166309822\n",
      "Exception\n",
      "_num_factors 10 _num_iter 10 0.01 inf\n",
      "_num_factors 10 _num_iter 30 0.001 0.459284790332\n",
      "Exception\n",
      "_num_factors 10 _num_iter 30 0.01 inf\n",
      "_num_factors 10 _num_iter 50 0.001 0.553311179927\n",
      "Exception\n",
      "_num_factors 10 _num_iter 50 0.01 inf\n",
      "_num_factors 10 _num_iter 100 0.001 0.557129329804\n",
      "Exception\n",
      "_num_factors 10 _num_iter 100 0.01 inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_fac</th>\n",
       "      <th>num_iter</th>\n",
       "      <th>initial_learning_rate</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.475593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.393412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.536196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.441353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.530532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.43209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.534203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.44346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.535219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.424166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.495771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.338617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.460684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.494094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.47481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.501676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.515469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.512112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.532584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.488529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.414517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.546238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.556605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.558211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.445516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.283978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.403085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.398137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.336637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.379167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.481285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.44457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.477547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.449571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.494966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.419115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.50087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.524826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.527706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.226239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.482166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.459285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.553311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.557129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_fac num_iter initial_learning_rate        r2\n",
       "0        1        2                 0.001  0.475593\n",
       "1        1        2                  0.01  0.393412\n",
       "2        1       10                 0.001  0.536196\n",
       "3        1       10                  0.01  0.441353\n",
       "4        1       30                 0.001  0.530532\n",
       "5        1       30                  0.01   0.43209\n",
       "6        1       50                 0.001  0.534203\n",
       "7        1       50                  0.01   0.44346\n",
       "8        1      100                 0.001  0.535219\n",
       "9        1      100                  0.01  0.424166\n",
       "10       2        2                 0.001  0.495771\n",
       "11       2        2                  0.01  0.338617\n",
       "12       2       10                 0.001  0.460684\n",
       "13       2       10                  0.01  0.494094\n",
       "14       2       30                 0.001   0.47481\n",
       "15       2       30                  0.01  0.501676\n",
       "16       2       50                 0.001  0.515469\n",
       "17       2       50                  0.01  0.512112\n",
       "18       2      100                 0.001  0.532584\n",
       "19       2      100                  0.01  0.488529\n",
       "20       3        2                 0.001  0.414517\n",
       "21       3        2                  0.01       inf\n",
       "22       3       10                 0.001  0.546238\n",
       "23       3       10                  0.01       inf\n",
       "24       3       30                 0.001    0.5593\n",
       "25       3       30                  0.01       inf\n",
       "26       3       50                 0.001  0.556605\n",
       "27       3       50                  0.01       inf\n",
       "28       3      100                 0.001  0.558211\n",
       "29       3      100                  0.01       inf\n",
       "..     ...      ...                   ...       ...\n",
       "70       8        2                 0.001  0.445516\n",
       "71       8        2                  0.01  0.283978\n",
       "72       8       10                 0.001  0.403085\n",
       "73       8       10                  0.01  0.398137\n",
       "74       8       30                 0.001  0.336637\n",
       "75       8       30                  0.01  0.379167\n",
       "76       8       50                 0.001  0.481285\n",
       "77       8       50                  0.01   0.44457\n",
       "78       8      100                 0.001  0.477547\n",
       "79       8      100                  0.01  0.449571\n",
       "80       9        2                 0.001  0.494966\n",
       "81       9        2                  0.01       inf\n",
       "82       9       10                 0.001  0.419115\n",
       "83       9       10                  0.01       inf\n",
       "84       9       30                 0.001   0.50087\n",
       "85       9       30                  0.01       inf\n",
       "86       9       50                 0.001  0.524826\n",
       "87       9       50                  0.01       inf\n",
       "88       9      100                 0.001  0.527706\n",
       "89       9      100                  0.01       inf\n",
       "90      10        2                 0.001  0.226239\n",
       "91      10        2                  0.01       inf\n",
       "92      10       10                 0.001  0.482166\n",
       "93      10       10                  0.01       inf\n",
       "94      10       30                 0.001  0.459285\n",
       "95      10       30                  0.01       inf\n",
       "96      10       50                 0.001  0.553311\n",
       "97      10       50                  0.01       inf\n",
       "98      10      100                 0.001  0.557129\n",
       "99      10      100                  0.01       inf\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31min 51s\n"
     ]
    }
   ],
   "source": [
    "grid_search = {'num_factors':np.arange(1,11),\n",
    "    'num_iter':[2,10,30,50,100],\n",
    "    'initial_learning_rate':[0.001, 0.01]\n",
    "    }\n",
    "df_grid_res = pd.DataFrame(columns=['num_fac', 'num_iter', 'initial_learning_rate', 'r2'])\n",
    "i = 0\n",
    "for _num_factors, _num_iter, _initial_learning_rate  in itertools.product( *grid_search.values() ):\n",
    "    fm = pylibfm.FM(num_factors=_num_factors,\n",
    "                    num_iter=_num_iter,\n",
    "                    verbose=False,\n",
    "                    task=\"regression\",\n",
    "                    initial_learning_rate=_initial_learning_rate,\n",
    "                    validation_size=0.01,\n",
    "                    learning_rate_schedule=\"optimal\")\n",
    "    fm.fit(X_train, y_train)\n",
    "    try:\n",
    "        r2 = r2_score(y_test, fm.predict(X_test))\n",
    "    except:\n",
    "        print('Exception')\n",
    "        r2 = np.Inf\n",
    "    #print('_num_factors',_num_factors, '_num_iter', _num_iter, _initial_learning_rate, r2)\n",
    "    print(i, r2)\n",
    "    df_grid_res.loc[i,:] = [_num_factors, _num_iter, _initial_learning_rate, r2]\n",
    "    i=i+1\n",
    "    \n",
    "#df_grid_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subaevdi/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b31bf7c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 234 ms\n"
     ]
    }
   ],
   "source": [
    "bind = df_grid_res.r2 != np.Inf\n",
    "#plt.plot(df_grid_res[bind].sort_values(\"r2\", ascending=False).r2.values, 'ro')\n",
    "plt.hist(df_grid_res[bind].sort_values(\"r2\", ascending=False).r2.values)\n",
    "bind = (df_grid_res.r2 > 0.5) & (df_grid_res.r2 != np.Inf)\n",
    "df_grid_FM_top = df_grid_res[bind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.36 ms\n"
     ]
    }
   ],
   "source": [
    "df_grid_res\n",
    "\n",
    "if not os.path.isfile('./../tmp/df_grid_res_FFM.pkl'):\n",
    "    with open('./../tmp/df_grid_res_FFM.pkl', 'wb') as f:\n",
    "        pickle.dump(df_grid_res, f)\n",
    "else:\n",
    "    with open('./../tmp/df_grid_res_FFM.pkl', 'rb') as f:\n",
    "        print(\"Load from file...\")\n",
    "\n",
    "        df_grid_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.18 ms\n"
     ]
    }
   ],
   "source": [
    "df_grid_FM_top.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 57.8 s\n"
     ]
    }
   ],
   "source": [
    "res_array = np.zeros((len(y), 30))\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "df_grid_FM_top['r2_kf'] = 0\n",
    "\n",
    "i = 0\n",
    "for ind, row  in df_grid_FM_top.iterrows():\n",
    "    _num_factors = row['num_fac']\n",
    "    _num_iter = row['num_iter']\n",
    "    _initial_learning_rate = row['initial_learning_rate']\n",
    "    res_cv = y.copy()\n",
    "    res_cv[:] = np.mean(y)\n",
    "    fm = pylibfm.FM(num_factors=_num_factors,\n",
    "                        num_iter=_num_iter,\n",
    "                        verbose=False,\n",
    "                        task=\"regression\",\n",
    "                        initial_learning_rate=_initial_learning_rate,\n",
    "                        validation_size=0.01,\n",
    "                        learning_rate_schedule=\"optimal\")\n",
    "\n",
    "    for train_index, test_index in kf.split(tmp):\n",
    "        X_train, X_test = tmp[train_index], tmp[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        #print(X_train.shape, X_test.shape)\n",
    "        fm.fit(X_train, y_train)\n",
    "        try:\n",
    "            res_cv[test_index] = fm.predict(X_test)\n",
    "        except:\n",
    "            print(\"Prediction exception\")\n",
    "    \n",
    "    df_grid_FM_top.loc[ind, 'r2_kf'] = r2_score(y, res_cv)\n",
    "    res_array[:,i] = res_cv.values\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       130.81\n",
       "1        88.53\n",
       "2        76.26\n",
       "3        80.62\n",
       "4        78.02\n",
       "5        92.93\n",
       "6       128.76\n",
       "7        91.91\n",
       "8       108.67\n",
       "9       126.99\n",
       "10      102.09\n",
       "11       98.12\n",
       "12       82.62\n",
       "13       94.12\n",
       "14       99.15\n",
       "15       93.64\n",
       "16      106.10\n",
       "17      114.13\n",
       "18       89.81\n",
       "19       90.81\n",
       "20       90.56\n",
       "21       94.57\n",
       "22      108.14\n",
       "23      120.77\n",
       "24       84.84\n",
       "25       93.59\n",
       "26      104.07\n",
       "27       89.37\n",
       "28       90.08\n",
       "29      128.19\n",
       "         ...  \n",
       "4179     85.93\n",
       "4180     90.45\n",
       "4181     90.06\n",
       "4182     90.38\n",
       "4183     95.56\n",
       "4184    109.00\n",
       "4185    109.64\n",
       "4186    131.98\n",
       "4187     98.15\n",
       "4188    102.33\n",
       "4189    102.42\n",
       "4190     89.11\n",
       "4191     88.93\n",
       "4192    103.03\n",
       "4193    107.24\n",
       "4194     91.13\n",
       "4195     86.23\n",
       "4196     99.93\n",
       "4197     89.25\n",
       "4198     97.09\n",
       "4199     88.24\n",
       "4200    108.59\n",
       "4201    107.39\n",
       "4202    123.34\n",
       "4203     85.71\n",
       "4204    107.39\n",
       "4205    108.77\n",
       "4206    109.22\n",
       "4207     87.48\n",
       "4208    110.85\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.62 ms\n"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
